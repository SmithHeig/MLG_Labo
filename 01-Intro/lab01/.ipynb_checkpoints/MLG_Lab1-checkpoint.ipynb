{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG / Introduction to jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this laboratory you are going to learn how to perform interactive computing using **jupyter**. The **jupyter** project, born out of the IPython Project in 2014, evolved to support interactive data science and scientific computing across all programming languages. It was initially an interactive shell for python that has more functionnalities than the basic one, now it allows you to interact with your scripts using so called **notebooks**. The notebook extends the console-based approach to interactive computing in a qualitatively new direction, providing a web-based application suitable for capturing the whole computation process: developing, documenting, and executing code, as well as communicating the results.\n",
    "\n",
    "This guide does not start from the basics of the general purpose language **python**. If you do not know this language, it is recommended to follow a **python** tutorial in order to learn the basic concepts and commands. You can have a look at the [official python tutorial](https://docs.python.org/2/tutorial/) or [Google's python tutorial](https://developers.google.com/edu/python/) for example.\n",
    "\n",
    "Note that for this course, we will use the Python 3.X series.\n",
    "\n",
    "You will use a browser-based notebook to interactively explore a dataset by:\n",
    "- Reading raw data from ascii files\n",
    "- Reading typed data (data frames) from ascii files\n",
    "- Selecting specific columns and/or rows from a dataset\n",
    "- Filtering datasets\n",
    "- Plotting the information in the dataset (e.g., scatter-plot, boxplot, histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using the notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are in an jupyter notebook right now. A jupyter notebook is a web interface to a python interpreter.\n",
    "\n",
    "A notebook is made of cells. Each cell has a type which defines what happens when it is run. \n",
    "\n",
    "- Markdown cells allow you to write [Markdown](http://daringfireball.net/projects/markdown/) text in them. They are just displayed as HTML when run.\n",
    "- Code cells contain python code. When the cell is run, the code is sent to the python interpreter, executed and you get the result in the cell output.\n",
    "- Various header cells that allow you to structure your document.\n",
    "\n",
    "You can change the type of a cell using the drop-down menu in the toolbar.\n",
    "\n",
    "You can click (for Code cells) or double-click (for headers and markdown cells) on cells to edit their content. You can then use keyboard shortcuts to run them :\n",
    "\n",
    "- Ctrl + Enter : run in place\n",
    "- Shift + Enter : run and move to next cell\n",
    "- Alt + Enter : run and insert new cell after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# This is a code cell containing python code !\n",
    "print( 2 + 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python interpreter that executes the code you write in the notebook is called a *Kernel*. You can restart the kernel (the interpreter) using the *Kernel* menu. This is useful if you want to delete all your variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter has also \"magic\" functions that start with % . They allow you to do a lot of useful things with your ipython environment :\n",
    "\n",
    "http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb\n",
    "\n",
    "The %who magic gives you a list of the defined python variables. object? can be used to get documentation about an object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t \n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "%who"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scientific computing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "This is the traditional python help() function :\n",
      "\n",
      "Help on function my_documented_function in module __main__:\n",
      "\n",
      "my_documented_function(a)\n",
      "    This is a revolutionary function that returns a + 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_documented_function(a):\n",
    "    '''\n",
    "    This is a revolutionary function that returns a + 1\n",
    "    '''\n",
    "    return a + 1\n",
    "\n",
    "print(my_documented_function(2))\n",
    "print('This is the traditional python help() function :\\n')\n",
    "help(my_documented_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the same info with just ? (note that you have to run this cell to view the effect)\n",
    "my_documented_function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a number of packages (libraries) dedicated to scientific programming :\n",
    "\n",
    "The foundation is [numpy](http://www.numpy.org/) which provides a N-dimensional array implementation with a nice indexing syntax (similar to MATLAB).\n",
    "\n",
    "Then comes [scipy](http://www.scipy.org/) which contains a number of algorithms (signal processing, distance computation, etc...) built on top of numpy.\n",
    "\n",
    "[matplotlib](http://matplotlib.org/) is a library to create 2D plots.\n",
    "\n",
    "[pandas](http://pandas.pydata.org/) provides a DataFrame implementation, which is a layer on top of numpy arrays that makes some things (handling missing values, date indexing) easier. Heavily inspired by the [R](http://www.r-project.org/) statistical computing language.\n",
    "\n",
    "[scikit-learn](http://scikit-learn.org/stable/) is a machine learning library that contains implementations of many of the most popular machine learning algorithms.\n",
    "\n",
    "[keras](https://keras.io/) and [tensorflow](http://www.tensorflow.org) allow you to write programs that are compiled and can run on a GPU.\n",
    "\n",
    "Finally, this is not a python package, but [stackoverflow](http://stackoverflow.com/) is a really good questions and answers platform where you can probably find answers to the most common problems you'll have :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to install a scientific python environment on your machines, we suggest the use of [anaconda](https://store.continuum.io/cshop/anaconda/). It is a \"python distribution\" that comes with a package manager (conda) and all of the scientific packages listed above (and many others) pre-installed. We strongly recommend to use the [miniconda](https://docs.conda.io/en/latest/miniconda.html) variant, which install only the basic packages and the package manager (conda) while consuming less space in your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick numpy introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy allows you to define [multidimensionnal arrays](http://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) (recommended reading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Makes the numpy function available as np.<funcname> (np is a convention)\n",
    "import numpy as np\n",
    "\n",
    "array1 = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]]\n",
    ")\n",
    "\n",
    "print(array1.shape)\n",
    "\n",
    "# The last line of a python cell is evaluated and used as the output for the cell\n",
    "array1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array elements can be accessed using the [indexing syntax](http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#arrays-indexing) (recommended reading). Numpy (and python) uses 0-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element (2,3) :  6\n",
      "first row     :  [1 2 3]\n",
      "second column :  [2 5]\n",
      "second and third element of the second row :  [5 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"element (2,3) : \", array1[1,2])\n",
    "print(\"first row     : \", array1[0,:])\n",
    "print(\"second column : \", array1[:,1])\n",
    "print(\"second and third element of the second row : \", array1[1, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a [lot of things](http://docs.scipy.org/doc/numpy/reference/) with numpy arrays. For example, we can compute the mean of each row :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 5.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort an array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\t :\t [5 8 2 9 4 3]\n",
      "using argsort\t :\t [2 3 4 5 8 9]\n",
      "after sort()\t :\t [2 3 4 5 8 9]\n"
     ]
    }
   ],
   "source": [
    "array2 = np.array([5, 8, 2, 9, 4, 3])\n",
    "print(\"original\\t :\\t\", array2)\n",
    "\n",
    "\n",
    "print(\"using argsort\\t :\\t\", array2[np.argsort(array2)]) # the fancy indexing version\n",
    "\n",
    "array2.sort() # note that this does in-place sorting, so it *modifies* array2\n",
    "print(\"after sort()\\t :\\t\", array2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can randomly shuffle an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 8 5 9 3]\n",
      "the max value is at position: 4\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(array2)\n",
    "\n",
    "print(array2)\n",
    "\n",
    "print(\"the max value is at position:\", np.argmax(array2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use comparison operators on arrays, giving us a boolean mask. And then use the mask to index the array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[False False  True]\n",
      " [ True False False]]\n",
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "mask = (array1 > 2) & (array1 < 5)\n",
    "print(array1)\n",
    "print(mask)\n",
    "print(array1[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Exercises</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As exercises, do the following :\n",
    "\n",
    "- Compute the max of each row of array1\n",
    "- Compute the max of each column of array1\n",
    "- Print the elements of array1 that are less than 4 (without using a loop)\n",
    "- Convert the range of the values in array2 to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-349f0595296e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-349f0595296e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    each row of array1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Compute the max of each row of array1\n",
    "print(np.max(array1, axis=1))\n",
    "\n",
    "# Compute the max of each column of array1\n",
    "print(np.max(array1, axis=0))\n",
    "\n",
    "# Print the elements of array1 that are less than 4 (without using a loop)\n",
    "print(array1[array1 < 4])\n",
    "\n",
    "# Convert the range of the values in array2 to [0,1]\n",
    "print(array2)\n",
    "print(np.interp(array2,(array2.min(), array2.max()),(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a dataset with numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the wine dataset from the UCI repository :\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "\n",
    "The wine.data file is a simple CSV file, which we can be easily loaded as a numpy array with np.genfromtxt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some nicer printing defaults for numpy arrays\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "# Be careful to place the database in the right directory or adapt the path\n",
    "data = np.genfromtxt('data/wine/wine.data', delimiter=',')\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a dataset with 178 samples and 14 dimensions for each sample. Let's have a look at the dataset description file (cat is a nice jupyter command that prints the content of a text file, like the unix cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat 'data/wine/wine.names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the column names in an array\n",
    "colnames = np.array([\n",
    "    'class', 'alcohol', 'malic acid', 'ash', 'alcalinity of ash', 'magnesium', 'total phenols', \n",
    "    'flavanoids', 'nonflavanoid phenols', 'proanthocyanins', 'color intensity', 'hue',\n",
    "    'OD280/OD315 of diluted wines', 'proline'\n",
    "])\n",
    "colnames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick introduction to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have two numpy arrays. *data* contains our dataset and *colnames* the name of our columns. However, we would like to deal with only one data structure that can store both the data and the columns names. That's what pandas' DataFrame are for ! (or, but a bit less elegant, [numpy structured arrays](http://docs.scipy.org/doc/numpy/user/basics.rec.html)).\n",
    "\n",
    "(Note that we could also directly load our CSV using pandas.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pd is a convention too\n",
    "\n",
    "df = pd.DataFrame(data=data[:,1:], columns=colnames[1:])\n",
    "# Ensure the class column is an int\n",
    "df['class'] = data[:,0].astype(np.int)\n",
    "\n",
    "# Pandas dataframes have a nice pretty-printing for ipython notebooks\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames have a nice <b>describe()</b> function that print some per-column statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Exercise</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in statistical terms the alcohol variable for each class of wine. E.g., use the describe() function of dataframes for each class of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['class'])['alcohol'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visual exploratory analysis of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, exploratory data analysis (EDA) is an approach for analyzing datasets to summarize their main characteristics, often with visual methods.\n",
    "\n",
    "For example, box and whisker plots use a graphical box: the bottom and top of the box are always the first (Q1) and third (Q3) quartiles, and the band inside the box is always the second quartile (the median). The whiskers are placed at Q1 - 1.5 IQR and Q3 + 1.5 IQR, where IQR means Inter-quartile range.\n",
    "See [Boxplot definition at Wikipedia](https://en.wikipedia.org/wiki/Box_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as pl # pylab is matplotlib\n",
    "# The line below enables matplotlib-jupyter integration and allows plots to be displayed inline in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see what the distribution of each feature is for each class. We'll use the [boxplot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.boxplot) function of pylab/pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(df['class'])\n",
    "print('classes : ', classes)\n",
    "\n",
    "alcohol_by_class = [df['alcohol'][df['class'] == c] for c in classes]\n",
    "\n",
    "pl.boxplot(alcohol_by_class)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframes have a [boxplot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html) function that does per-class plotting with the *by* parameter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='alcohol', by='class');    # the ; at the end makes Python not to print the response of boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do that for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    df.boxplot(column=c, by='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: By looking at the boxplot, which features seem the most discriminative ? (which variables would be helpful to separate the wine classes?)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenols and flavanoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classifying the wine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try a rule-based approach to classify the wine data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the <b>alcohol</b> variable allows for a good classification and by observing the corresponding boxplot, let's define the rules that associate a class to alcohol range values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "for row in df['alcohol']:\n",
    "    if row > 13.5:\n",
    "        pred.append(1);\n",
    "    elif row > 12.75 and row < 13.5 :\n",
    "        pred.append(2);    \n",
    "    else:\n",
    "        pred.append(3)\n",
    "\n",
    "# A new column is added to the dataframe\n",
    "df['prediction'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Can you estimate the performance of such a classification method ?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['eq'] = np.where((df['class'] == df['prediction']), 1, 0)\n",
    "goodResult = df['eq'].sum()\n",
    "nbEntries = df['eq'].count()\n",
    "performance = (goodResult /  nbEntries) * 100\n",
    "print(str(performance) + ' %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hint:</b> Divide the number of times the prediction value corresponds to the real one (e.g., how many times, class == prediction ?), by the total number of observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix, precision, recall and F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>confusion matrix</b> is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. In the confusion matrix all correct guesses are located in the diagonal of the table, so it's easy to visually inspect the table for errors, as they will be represented by values outside the diagonal.\n",
    "\n",
    "For an example, please see the Wikipedia page: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "Behind the confusion matrix are the following concepts:\n",
    "\n",
    "<b>true positives (TP)</b>: These are cases in which we predicted correctly the positive class (e.g. the person has a disease and has been diagnosed as sick).\n",
    "\n",
    "<b>true negatives (TN):</b> We correctly predicted the negative class (e.g. the person is healthy and has not been diagnosed as sick).\n",
    "\n",
    "<b>false positives (FP):</b> We wrongly predicted the positive class (e.g. the person don't actually have the disease but has been diagnosed as sick). (Also known as a \"Type I error.\")\n",
    "\n",
    "<b>false negatives (FN):</b> We wrongly predicted the negative class (e.g. the person si diagnosed as healthy but actually is sick). (Also known as a \"Type II error.\")\n",
    "\n",
    "<b>precision</b>: When we predict the positive class, how often are we correct?<p>\n",
    "\n",
    "<font color=\"red\">precision = tp/(tp + fp)</font>\n",
    "\n",
    "Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as classification function:\n",
    "\n",
    "<b>Recall or sensitivity</b> (also called the true positive rate or probability of detection in some fields) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).<p>\n",
    "\n",
    "<font color=\"red\">recall = tp/(tp + fn)</font>\n",
    "\n",
    "<b>Specificity</b> (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).\n",
    "\n",
    "The <b>F1 score</b> can be interpreted as a weighted average of precision and recall, where an F1 score reaches its best value at 1 and worst at 0.<p>\n",
    "\n",
    "<font color=\"red\">f1-score = 2 x precision x recall / ( precision + recall)</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates a colored confusion matrix.\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_confusion_matrix(confmat, labels_names, ax=None):\n",
    "    if ax is None:\n",
    "        ax = pl.subplot(111)\n",
    "    cmim = ax.matshow(confmat, interpolation='nearest', cmap=cm.jet)\n",
    "\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.annotate(str(confmat[i, j]), xy=(j, i),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        fontsize=8)\n",
    "    ax.set_xticks(np.arange(confmat.shape[0]))\n",
    "    ax.set_xticklabels([labels_names[l] for l in range(confmat.shape[0])], rotation='vertical')\n",
    "    ax.set_yticks(np.arange(confmat.shape[1]))\n",
    "    _ = ax.set_yticklabels([labels_names[l] for l in range(confmat.shape[1])])\n",
    "    ax.set_xlabel('predicted label')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.set_ylabel('true label')\n",
    "    pl.colorbar(cmim, shrink=0.7, orientation='horizontal', pad=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "labels_names=['1', '2', '3']\n",
    "C = skmetrics.confusion_matrix(y_true=df['class'], y_pred=df['prediction'])\n",
    "plot_confusion_matrix(C, labels_names)\n",
    "\n",
    "print(skmetrics.classification_report(y_true=df['class'], y_pred=df['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Define a rule that uses the most discriminative feature to classify the wine observations ?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "for row in df['flavanoids']:\n",
    "    if row >= 2.5775:\n",
    "        pred.append(1);\n",
    "    elif row > 1.2625 and row < 2.5775 :\n",
    "        pred.append(2);    \n",
    "    else:\n",
    "        pred.append(3)\n",
    "\n",
    "print(df.groupby('class')['flavanoids'].describe())\n",
    "# A new column is added to the dataframe\n",
    "df['prediction2'] = pred\n",
    "# calculate prescision\n",
    "df['eq2'] = np.where((df['class'] == df['prediction2']), 1, 0)\n",
    "goodResult = df['eq2'].sum()\n",
    "nbEntries = df['eq2'].count()\n",
    "performance = (goodResult /  nbEntries) * 100\n",
    "print(str(performance) + ' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Compute the confusion matrix of the resulting rule-based system ?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names=['1', '2', '3']\n",
    "C = skmetrics.confusion_matrix(y_true=df['class'], y_pred=df['prediction2'])\n",
    "plot_confusion_matrix(C, labels_names)\n",
    "\n",
    "print(skmetrics.classification_report(y_true=df['class'], y_pred=df['prediction2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Compute the precision, the recall and the f1-score of the system for a given class using the values of the confusion matrix ?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Try a simple machine learning classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "As you can see from the df.describe() above, the range values for each column in our dataset varies a lot. Depending on the model you want to use, that can be a big problem.\n",
    "\n",
    "The model we'll use here is k-Nearest Neighbor with the Euclidean distance. When using the Euclidean distance, it is important to think about how to preprocess your data.\n",
    "\n",
    "For example, look at the 'magnesium' and 'total phenols' columns. The standard deviation for magnesium is 14 while for total phenols it is 0.62. This means that the data are more spread out on the magnesium axis compared to the phenols axis. And if we use raw values to compute distances, the magnesium axis will be much more important than the phenols axis, but this importance will just be due to the (arbitrary) scales that we used to measure magnesium and phenols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['magnesium'].describe())\n",
    "print(df['total phenols'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the distances less dependent on particular scales, we can standardize our data by making sure each column has 0 mean and unit variance, using [sklearn.preprocessing.scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) or by computing the normalization ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# all variables of the dataset with normalization\n",
    "#X = scale(df[colnames[1:]])\n",
    "\n",
    "# some selected variables without normalization\n",
    "X = df[['magnesium', 'total phenols']].values\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the datasets for cross-validation: Train/test split\n",
    "To build and evaluate a machine learning model, we need to split our data into training and testing sets. Scikit-learn has a [cross_validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) module that helps with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train and test are indices arrays containing the indices of train/test samples\n",
    "train, test = train_test_split(\n",
    "    np.arange(X.shape[0]), test_size=0.4, random_state=42 # we fix a random state for reproducibility\n",
    ")\n",
    "\n",
    "print(\"train shape : \", train.shape)\n",
    "print(\"test shape  : \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the wine data\n",
    "We will use a k-Nearest Neighbor classifier. This classifier will classify a new sample by assigning it the class of its nearest neighbor (for k=1). It computes the distance between the new sample and all the samples in the training set, find the nearest training sample and then use the class of the nearest neighbor to classify the new sample.\n",
    "\n",
    "The [scipy.spatial.distance](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html) module helps with distance computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return np.sqrt(x.dot(x))\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"Euclidean distance\"\"\"\n",
    "    return norm(x1 - x2)\n",
    "\n",
    "def distances(X_train, x):\n",
    "    \"\"\"\n",
    "    Returns a vector containing the distance between x and each\n",
    "    sample in X_train\n",
    "    \"\"\"\n",
    "    n_train = X_train.shape[0]\n",
    "    return np.array([euclidean_distance(X_train[i,:], x) for i in range(n_train)])\n",
    "\n",
    "def nn_classify_single(X_train, y_train, X):\n",
    "    \"\"\"\n",
    "    Nearest neighbor classifier. Returns the class of the nearest training vector\n",
    "    \"\"\"\n",
    "    dists = distances(X_train, X)\n",
    "    closest = np.argmin(dists)\n",
    "    return y_train[closest]\n",
    "\n",
    "def nn_classify(X_train, y_train, X):\n",
    "    \"\"\"\n",
    "    Assign to each row in X the class of its nearest neighbor in X_train\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[0], dtype=np.int)\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred[i] = nn_classify_single(X_train, y_train, X[i])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nn_classify(X[train], y[train], X[test])\n",
    "print(\"y_true : \", y[test])\n",
    "print(\"y_pred : \", y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names=['1', '2', '3']\n",
    "C = skmetrics.confusion_matrix(y_true=y[test], y_pred=y_test_pred)\n",
    "plot_confusion_matrix(C, labels_names)\n",
    "\n",
    "print(skmetrics.classification_report(y_true=y[test], y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Now, normalize (scale) the columns of your data matrix and re-run the classification. What do you observe ?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['magnesium', 'total phenols']].values\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] = X[:,i] / X[:,i].max()\n",
    "    \n",
    "train, test = train_test_split(\n",
    "    np.arange(X.shape[0]), test_size=0.4, random_state=42 # we fix a random state for reproducibility\n",
    ")\n",
    "y_test_pred = nn_classify(X[train], y[train], X[test])\n",
    "print(\"y_true : \", y[test])\n",
    "print(\"y_pred : \", y_test_pred)\n",
    "labels_names=['1', '2', '3']\n",
    "C = skmetrics.confusion_matrix(y_true=y[test], y_pred=y_test_pred)\n",
    "plot_confusion_matrix(C, labels_names)\n",
    "\n",
    "print(skmetrics.classification_report(y_true=y[test], y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Above, we implemented a basic nearest neighbor classifier (k=1). Modify it so that it is a k nearest neighbor classifier. Evaluate it for k=3, k=5 and k=10. Does increasing k help ? Is there a limit after which increasing k is useless ? Why ?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "NB_NEIGHBOR = 3\n",
    "\n",
    "def nn_classify_single_k(X_train, y_train, X, k):\n",
    "    \"\"\"\n",
    "    Nearest neighbor classifier. Returns the class of the nearest training vector\n",
    "    \"\"\"\n",
    "    dists = distances(X_train, X)\n",
    "    pos = dists.argsort()[:k]\n",
    "    classVotes = {}\n",
    "    for i in range(k):\n",
    "        val = y_train[pos[i]]\n",
    "        if(val in classVotes):\n",
    "            classVotes[val] += 1\n",
    "        else:\n",
    "            classVotes[val] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "def nn_classify_k(X_train, y_train, X, k):\n",
    "    \"\"\"\n",
    "    Assign to each row in X the class of its nearest neighbor in X_train\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[0], dtype=np.int)\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred[i] = nn_classify_single_k(X_train, y_train, X[i], k)\n",
    "    return y_pred\n",
    "\n",
    "y_test_pred = nn_classify_k(X[train], y[train], X[test], NB_NEIGHBOR)\n",
    "print(\"y_true : \", y[test])\n",
    "print(\"y_pred : \", y_test_pred)\n",
    "\n",
    "labels_names=['1', '2', '3']\n",
    "C = skmetrics.confusion_matrix(y_true=y[test], y_pred=y_test_pred)\n",
    "plot_confusion_matrix(C, labels_names)\n",
    "\n",
    "print(skmetrics.classification_report(y_true=y[test], y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: In our example, we only used two features (magnesium and total phenols). Try classifying with all the features and compare the performance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_NEIGHBOR = 3\n",
    "X = df.values\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] = X[:,i] / X[:,i].max()\n",
    "train, test = train_test_split(\n",
    "    np.arange(X.shape[0]), test_size=0.4, random_state=42 # we fix a random state for reproducibility\n",
    ")\n",
    "y_test_pred = nn_classify_k(X[train], y[train], X[test], NB_NEIGHBOR)\n",
    "print(\"y_true : \", y[test])\n",
    "print(\"y_pred : \", y_test_pred)\n",
    "\n",
    "labels_names=['1', '2', '3']\n",
    "C = skmetrics.confusion_matrix(y_true=y[test], y_pred=y_test_pred)\n",
    "plot_confusion_matrix(C, labels_names)\n",
    "\n",
    "print(skmetrics.classification_report(y_true=y[test], y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
